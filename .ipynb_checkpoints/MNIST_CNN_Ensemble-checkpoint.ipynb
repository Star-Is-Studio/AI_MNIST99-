{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P04GO_STzCLb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "np.random.seed(80013493)\n",
    "tf.set_random_seed(80013493)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8oiD5WMcmba"
   },
   "source": [
    "mnist data를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9522,
     "status": "ok",
     "timestamp": 1544375688725,
     "user": {
      "displayName": "김현우",
      "photoUrl": "",
      "userId": "07383167550908655617"
     },
     "user_tz": -540
    },
    "id": "LIaFCs00zjtL",
    "outputId": "6f9fbf9f-c6fb-4bca-92c3-0146e8f9d1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-1854b1a1becf>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_akr_4-cuTB"
   },
   "source": [
    "첫 번째 단계의 합성곱필터와 폴링계층을 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-_en_FX7lfj"
   },
   "outputs": [],
   "source": [
    "num_filters1 = 32\n",
    "num_filters2 = 64\n",
    "\n",
    "num_units1 = 7*7*num_filters2\n",
    "num_units2 = 1024\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = tf.get_variable(\"w_conv1\",[5,5,1,num_filters1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "h_conv1 = tf.nn.conv2d(x_image, W_conv1,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
    "h_conv1_cutoff = tf.nn.relu(h_conv1 + b_conv1)\n",
    "\n",
    "h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFU-j90Oc8QH"
   },
   "source": [
    "두 번째 단계의 합성곱필터와 폴링계층을 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KdVgcmm9o-m"
   },
   "outputs": [],
   "source": [
    "\n",
    "W_conv2 = tf.get_variable(\"w_conv2\",[5,5,num_filters1, num_filters2],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "h_conv2 = tf.nn.conv2d(h_pool1, W_conv2,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[num_filters2]))\n",
    "h_conv2_cutoff = tf.nn.relu(h_conv2 + b_conv2)\n",
    "\n",
    "h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkhBGezhc_3K"
   },
   "source": [
    " 결합층, 드롭아웃 계층, 소프트맥스 함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Iisgmau-cqA"
   },
   "outputs": [],
   "source": [
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*num_filters2])\n",
    "\n",
    "\n",
    "w1 = tf.get_variable(\"w1\",[num_units1,num_units2],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[num_units2]))\n",
    "hidden2 = tf.nn.relu(tf.matmul(h_pool2_flat, w1) + b1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden2_drop = tf.nn.dropout(hidden2, keep_prob)\n",
    "\n",
    "w2 = tf.get_variable(\"w2\",[num_units2,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "p_1 = tf.nn.softmax(tf.matmul(hidden2_drop, w2) + b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAvs3CQYi3Oq"
   },
   "source": [
    "2번째 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzM4V3PChxpO"
   },
   "outputs": [],
   "source": [
    "x_2 = tf.placeholder(tf.float32, [None, 784])\n",
    "x_image_2 = tf.reshape(x_2, [-1,28,28,1])\n",
    "\n",
    "W_conv1_2 = tf.get_variable(\"w_conv1_2\",[5,5,1,num_filters1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "h_conv1_2 = tf.nn.conv2d(x_image_2, W_conv1_2,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "b_conv1_2 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
    "h_conv1_cutoff_2 = tf.nn.relu(h_conv1_2 + b_conv1_2)\n",
    "\n",
    "h_pool1_2 = tf.nn.max_pool(h_conv1_cutoff_2, ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "W_conv2_2 = tf.get_variable(\"w_conv2_2\",[5,5,num_filters1, num_filters2],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "h_conv2_2 = tf.nn.conv2d(h_pool1_2, W_conv2_2,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "b_conv2_2 = tf.Variable(tf.constant(0.1, shape=[num_filters2]))\n",
    "h_conv2_cutoff_2 = tf.nn.relu(h_conv2_2 + b_conv2_2)\n",
    "\n",
    "h_pool2_2 = tf.nn.max_pool(h_conv2_cutoff_2, ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n",
    "h_pool2_flat_2 = tf.reshape(h_pool2_2, [-1, 7*7*num_filters2])\n",
    "\n",
    "\n",
    "w1_2 = tf.get_variable(\"w1_2\",[num_units1,num_units2],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1_2 = tf.Variable(tf.constant(0.1, shape=[num_units2]))\n",
    "hidden2_2 = tf.nn.relu(tf.matmul(h_pool2_flat_2, w1_2) + b1_2)\n",
    "\n",
    "keep_prob_2 = tf.placeholder(tf.float32)\n",
    "hidden2_drop_2 = tf.nn.dropout(hidden2_2, keep_prob_2)\n",
    "\n",
    "w2_2 = tf.get_variable(\"w2_2\",[num_units2,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2_2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "p_2 = tf.nn.softmax(tf.matmul(hidden2_drop_2, w2_2) + b2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Txwf1SNQ8PAL"
   },
   "source": [
    "3번째모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuUfnWf98Obz"
   },
   "outputs": [],
   "source": [
    "x_3 = tf.placeholder(tf.float32, [None, 784])\n",
    "x_image_3 = tf.reshape(x_3, [-1,28,28,1])\n",
    "\n",
    "W_conv1_3 = tf.get_variable(\"w_conv1_3\",[5,5,1,num_filters1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "h_conv1_3 = tf.nn.conv2d(x_image_3, W_conv1_3,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "b_conv1_3 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
    "h_conv1_cutoff_3 = tf.nn.relu(h_conv1_3 + b_conv1_3)\n",
    "\n",
    "h_pool1_3 = tf.nn.max_pool(h_conv1_cutoff_3, ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "W_conv2_3 = tf.get_variable(\"w_conv2_3\",[5,5,num_filters1, num_filters2],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "h_conv2_3 = tf.nn.conv2d(h_pool1_3, W_conv2_3,strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "b_conv2_3 = tf.Variable(tf.constant(0.1, shape=[num_filters2]))\n",
    "h_conv2_cutoff_3 = tf.nn.relu(h_conv2_3 + b_conv2_3)\n",
    "\n",
    "h_pool2_3 = tf.nn.max_pool(h_conv2_cutoff_3, ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n",
    "h_pool2_flat_3 = tf.reshape(h_pool2_3, [-1, 7*7*num_filters2])\n",
    "\n",
    "\n",
    "w1_3 = tf.get_variable(\"w1_3\",[num_units1,num_units2],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1_3 = tf.Variable(tf.constant(0.1, shape=[num_units2]))\n",
    "hidden2_3 = tf.nn.relu(tf.matmul(h_pool2_flat_3, w1_3) + b1_3)\n",
    "\n",
    "keep_prob_3 = tf.placeholder(tf.float32)\n",
    "hidden2_drop_3 = tf.nn.dropout(hidden2_3, keep_prob_3)\n",
    "\n",
    "w2_3 = tf.get_variable(\"w2_3\",[num_units2,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2_3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "p_3 = tf.nn.softmax(tf.matmul(hidden2_drop_3, w2_3) + b2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Wj1jzObdEie"
   },
   "source": [
    "loss, optimizer, accuracy을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jDunLMGBoAX"
   },
   "outputs": [],
   "source": [
    "t_1 = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "t_2 = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "t_3 = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "loss_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=p_1, labels=t_1))\n",
    "\n",
    "loss_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=p_2, labels=t_2))\n",
    "\n",
    "loss_3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=p_3, labels=t_3))\n",
    "\n",
    "optimizer_1 = tf.train.AdamOptimizer(0.0001).minimize(loss_1)\n",
    "\n",
    "optimizer_2 = tf.train.AdamOptimizer(0.0001).minimize(loss_2)\n",
    "\n",
    "optimizer_3 = tf.train.AdamOptimizer(0.0001).minimize(loss_3)\n",
    "\n",
    "correct_prediction_1 = tf.equal(tf.argmax(p_1, 1), tf.argmax(t_1, 1))\n",
    "\n",
    "correct_prediction_2 = tf.equal(tf.argmax(p_2, 1), tf.argmax(t_2, 1))\n",
    "\n",
    "correct_prediction_3 = tf.equal(tf.argmax(p_3, 1), tf.argmax(t_3, 1))\n",
    "\n",
    "accuracy_1 = tf.reduce_mean(tf.cast(correct_prediction_1, tf.float32))\n",
    "\n",
    "accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, tf.float32))\n",
    "\n",
    "accuracy_3 = tf.reduce_mean(tf.cast(correct_prediction_3, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpMWRU0adN5-"
   },
   "source": [
    "세션을 준비한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmHy2zrsCNAE"
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_azWImjCdPsQ"
   },
   "source": [
    "### 학습을 30000회반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 639298,
     "status": "ok",
     "timestamp": 1544376318616,
     "user": {
      "displayName": "김현우",
      "photoUrl": "",
      "userId": "07383167550908655617"
     },
     "user_tz": -540
    },
    "id": "kS5KkfGTCWZX",
    "outputId": "f73f12ec-0da5-477e-85b1-28f7c625a781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss1: 1.492391, Accuracy1: 0.972700\n",
      "Step: 2000, Loss1: 1.480332, Accuracy1: 0.982600\n",
      "Step: 3000, Loss1: 1.475556, Accuracy1: 0.987100\n",
      "Step: 4000, Loss1: 1.473018, Accuracy1: 0.989600\n",
      "Step: 5000, Loss1: 1.471982, Accuracy1: 0.990300\n",
      "Step: 6000, Loss1: 1.471118, Accuracy1: 0.991200\n",
      "Step: 7000, Loss1: 1.469862, Accuracy1: 0.992300\n",
      "Step: 8000, Loss1: 1.469405, Accuracy1: 0.992200\n",
      "Step: 9000, Loss1: 1.469098, Accuracy1: 0.992900\n",
      "Step: 10000, Loss1: 1.468861, Accuracy1: 0.992800\n",
      "Step: 11000, Loss1: 1.468750, Accuracy1: 0.992600\n",
      "Step: 12000, Loss1: 1.469628, Accuracy1: 0.992000\n",
      "Step: 13000, Loss1: 1.469302, Accuracy1: 0.992100\n",
      "Step: 14000, Loss1: 1.469035, Accuracy1: 0.992100\n",
      "Step: 15000, Loss1: 1.469614, Accuracy1: 0.991900\n",
      "Step: 16000, Loss1: 1.469157, Accuracy1: 0.992500\n",
      "Step: 17000, Loss1: 1.468259, Accuracy1: 0.992900\n",
      "Step: 18000, Loss1: 1.467891, Accuracy1: 0.993500\n",
      "Step: 19000, Loss1: 1.468695, Accuracy1: 0.992300\n",
      "Step: 20000, Loss1: 1.467883, Accuracy1: 0.993200\n",
      "Step: 21000, Loss1: 1.469215, Accuracy1: 0.992000\n",
      "Step: 22000, Loss1: 1.468379, Accuracy1: 0.992800\n",
      "Step: 23000, Loss1: 1.468662, Accuracy1: 0.992500\n",
      "Step: 24000, Loss1: 1.468696, Accuracy1: 0.992500\n",
      "Step: 25000, Loss1: 1.468149, Accuracy1: 0.993000\n",
      "Step: 26000, Loss1: 1.468683, Accuracy1: 0.992500\n",
      "Step: 27000, Loss1: 1.467985, Accuracy1: 0.993500\n",
      "Step: 28000, Loss1: 1.468409, Accuracy1: 0.993100\n",
      "Step: 29000, Loss1: 1.468053, Accuracy1: 0.993000\n",
      "Step: 30000, Loss1: 1.468145, Accuracy1: 0.993100\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in range(30000):\n",
    "    i += 1\n",
    "    batch_xs, batch_ts = mnist.train.next_batch(200)\n",
    "    sess.run(optimizer_1,feed_dict={x:batch_xs, t_1:batch_ts, keep_prob:0.7})\n",
    "    if i % 1000 == 0:\n",
    "        loss_val_1, acc_val_1 = sess.run([loss_1, accuracy_1],feed_dict={x:mnist.test.images, t_1:mnist.test.labels,keep_prob:1.0})\n",
    "        print ('Step: %d, Loss1: %f, Accuracy1: %f'% (i, loss_val_1, acc_val_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqfeE3fOlrGu"
   },
   "source": [
    "모델2 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1470298,
     "status": "ok",
     "timestamp": 1544377149626,
     "user": {
      "displayName": "김현우",
      "photoUrl": "",
      "userId": "07383167550908655617"
     },
     "user_tz": -540
    },
    "id": "jr99eYp0eq9Q",
    "outputId": "5776ddbd-1991-40e2-b0bf-e7007d472347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss_2: 1.498463, Accuracy_2: 0.967300\n",
      "Step: 2000, Loss_2: 1.479417, Accuracy_2: 0.983600\n",
      "Step: 3000, Loss_2: 1.475733, Accuracy_2: 0.987400\n",
      "Step: 4000, Loss_2: 1.471937, Accuracy_2: 0.990300\n",
      "Step: 5000, Loss_2: 1.471949, Accuracy_2: 0.990300\n",
      "Step: 6000, Loss_2: 1.470610, Accuracy_2: 0.991100\n",
      "Step: 7000, Loss_2: 1.469580, Accuracy_2: 0.992000\n",
      "Step: 8000, Loss_2: 1.469315, Accuracy_2: 0.992000\n",
      "Step: 9000, Loss_2: 1.469269, Accuracy_2: 0.992300\n",
      "Step: 10000, Loss_2: 1.469125, Accuracy_2: 0.992300\n",
      "Step: 11000, Loss_2: 1.468915, Accuracy_2: 0.992900\n",
      "Step: 12000, Loss_2: 1.469291, Accuracy_2: 0.992200\n",
      "Step: 13000, Loss_2: 1.468354, Accuracy_2: 0.993200\n",
      "Step: 14000, Loss_2: 1.468511, Accuracy_2: 0.992900\n",
      "Step: 15000, Loss_2: 1.468926, Accuracy_2: 0.992300\n",
      "Step: 16000, Loss_2: 1.468694, Accuracy_2: 0.992600\n",
      "Step: 17000, Loss_2: 1.468195, Accuracy_2: 0.993400\n",
      "Step: 18000, Loss_2: 1.468502, Accuracy_2: 0.992700\n",
      "Step: 19000, Loss_2: 1.468664, Accuracy_2: 0.992700\n",
      "Step: 20000, Loss_2: 1.468881, Accuracy_2: 0.992400\n",
      "Step: 21000, Loss_2: 1.469505, Accuracy_2: 0.991900\n",
      "Step: 22000, Loss_2: 1.467989, Accuracy_2: 0.993300\n",
      "Step: 23000, Loss_2: 1.468743, Accuracy_2: 0.992500\n",
      "Step: 24000, Loss_2: 1.468998, Accuracy_2: 0.992200\n",
      "Step: 25000, Loss_2: 1.468858, Accuracy_2: 0.992500\n",
      "Step: 26000, Loss_2: 1.468191, Accuracy_2: 0.993100\n",
      "Step: 27000, Loss_2: 1.468148, Accuracy_2: 0.993300\n",
      "Step: 28000, Loss_2: 1.468728, Accuracy_2: 0.992200\n",
      "Step: 29000, Loss_2: 1.467987, Accuracy_2: 0.993400\n",
      "Step: 30000, Loss_2: 1.468492, Accuracy_2: 0.992800\n",
      "Step: 31000, Loss_2: 1.468973, Accuracy_2: 0.992000\n",
      "Step: 32000, Loss_2: 1.467819, Accuracy_2: 0.993300\n",
      "Step: 33000, Loss_2: 1.469165, Accuracy_2: 0.992100\n",
      "Step: 34000, Loss_2: 1.468408, Accuracy_2: 0.992600\n",
      "Step: 35000, Loss_2: 1.468502, Accuracy_2: 0.992300\n",
      "Step: 36000, Loss_2: 1.468444, Accuracy_2: 0.992600\n",
      "Step: 37000, Loss_2: 1.468178, Accuracy_2: 0.993200\n",
      "Step: 38000, Loss_2: 1.468499, Accuracy_2: 0.992600\n",
      "Step: 39000, Loss_2: 1.468434, Accuracy_2: 0.992500\n",
      "Step: 40000, Loss_2: 1.468281, Accuracy_2: 0.992900\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in range(40000):\n",
    "    i += 1\n",
    "    batch_xs_2, batch_ts_2 = mnist.train.next_batch(200)\n",
    "    sess.run(optimizer_2,feed_dict={x_2:batch_xs_2, t_2:batch_ts_2, keep_prob_2:0.7})\n",
    "    if i % 1000 == 0:\n",
    "        loss_val_2, acc_val_2 = sess.run([loss_2, accuracy_2],feed_dict={x_2:mnist.test.images, t_2:mnist.test.labels,keep_prob_2:1.0})\n",
    "        print ('Step: %d, Loss_2: %f, Accuracy_2: %f'% (i, loss_val_2, acc_val_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pURQUOOy7ySV"
   },
   "source": [
    "모델3학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2300364,
     "status": "ok",
     "timestamp": 1544377979703,
     "user": {
      "displayName": "김현우",
      "photoUrl": "",
      "userId": "07383167550908655617"
     },
     "user_tz": -540
    },
    "id": "d25eSVp97xlR",
    "outputId": "023da10c-6984-4a8a-9a1e-783a6205051a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss_3: 1.494051, Accuracy_3: 0.972100\n",
      "Step: 2000, Loss_3: 1.479801, Accuracy_3: 0.984000\n",
      "Step: 3000, Loss_3: 1.475346, Accuracy_3: 0.987500\n",
      "Step: 4000, Loss_3: 1.473593, Accuracy_3: 0.988900\n",
      "Step: 5000, Loss_3: 1.471024, Accuracy_3: 0.991300\n",
      "Step: 6000, Loss_3: 1.470162, Accuracy_3: 0.991700\n",
      "Step: 7000, Loss_3: 1.469415, Accuracy_3: 0.993000\n",
      "Step: 8000, Loss_3: 1.469257, Accuracy_3: 0.992600\n",
      "Step: 9000, Loss_3: 1.469599, Accuracy_3: 0.992100\n",
      "Step: 10000, Loss_3: 1.470544, Accuracy_3: 0.990900\n",
      "Step: 11000, Loss_3: 1.470440, Accuracy_3: 0.991400\n",
      "Step: 12000, Loss_3: 1.469109, Accuracy_3: 0.992600\n",
      "Step: 13000, Loss_3: 1.468320, Accuracy_3: 0.993500\n",
      "Step: 14000, Loss_3: 1.468314, Accuracy_3: 0.993100\n",
      "Step: 15000, Loss_3: 1.469576, Accuracy_3: 0.991600\n",
      "Step: 16000, Loss_3: 1.468678, Accuracy_3: 0.993100\n",
      "Step: 17000, Loss_3: 1.468633, Accuracy_3: 0.992100\n",
      "Step: 18000, Loss_3: 1.468678, Accuracy_3: 0.992700\n",
      "Step: 19000, Loss_3: 1.468935, Accuracy_3: 0.992100\n",
      "Step: 20000, Loss_3: 1.468829, Accuracy_3: 0.992500\n",
      "Step: 21000, Loss_3: 1.468100, Accuracy_3: 0.993300\n",
      "Step: 22000, Loss_3: 1.469396, Accuracy_3: 0.991900\n",
      "Step: 23000, Loss_3: 1.468274, Accuracy_3: 0.992700\n",
      "Step: 24000, Loss_3: 1.468417, Accuracy_3: 0.993100\n",
      "Step: 25000, Loss_3: 1.468715, Accuracy_3: 0.992500\n",
      "Step: 26000, Loss_3: 1.468459, Accuracy_3: 0.992900\n",
      "Step: 27000, Loss_3: 1.468019, Accuracy_3: 0.993200\n",
      "Step: 28000, Loss_3: 1.468907, Accuracy_3: 0.992300\n",
      "Step: 29000, Loss_3: 1.469538, Accuracy_3: 0.991300\n",
      "Step: 30000, Loss_3: 1.467927, Accuracy_3: 0.993400\n",
      "Step: 31000, Loss_3: 1.468080, Accuracy_3: 0.993100\n",
      "Step: 32000, Loss_3: 1.467444, Accuracy_3: 0.993700\n",
      "Step: 33000, Loss_3: 1.467881, Accuracy_3: 0.993200\n",
      "Step: 34000, Loss_3: 1.467729, Accuracy_3: 0.993600\n",
      "Step: 35000, Loss_3: 1.468402, Accuracy_3: 0.992400\n",
      "Step: 36000, Loss_3: 1.468532, Accuracy_3: 0.992200\n",
      "Step: 37000, Loss_3: 1.467743, Accuracy_3: 0.993500\n",
      "Step: 38000, Loss_3: 1.468204, Accuracy_3: 0.993100\n",
      "Step: 39000, Loss_3: 1.468165, Accuracy_3: 0.992900\n",
      "Step: 40000, Loss_3: 1.467864, Accuracy_3: 0.993300\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in range(40000):\n",
    "    i += 1\n",
    "    batch_xs_3, batch_ts_3 = mnist.train.next_batch(200)\n",
    "    sess.run(optimizer_3,feed_dict={x_3:batch_xs_3, t_3:batch_ts_3, keep_prob_3:0.7})\n",
    "    if i % 1000 == 0:\n",
    "        loss_val_3, acc_val_3 = sess.run([loss_3, accuracy_3],feed_dict={x_3:mnist.test.images, t_3:mnist.test.labels,keep_prob_3:1.0})\n",
    "        print ('Step: %d, Loss_3: %f, Accuracy_3: %f'% (i, loss_val_3, acc_val_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ks00NB_jAOV8"
   },
   "source": [
    "predict함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HIPR1aIu080Y"
   },
   "outputs": [],
   "source": [
    "def predict1(x_test,p):\n",
    "  return sess.run(p,feed_dict={x:x_test,keep_prob:1.0})\n",
    "def predict2(x_test,p):\n",
    "  return sess.run(p,feed_dict={x_2:x_test,keep_prob_2:1.0})\n",
    "def predict3(x_test,p):\n",
    "  return sess.run(p,feed_dict={x_3:x_test,keep_prob_3:1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHmvmjuGARF7"
   },
   "source": [
    "model1에 대한 predict 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPnNJ8LBxwpY"
   },
   "outputs": [],
   "source": [
    "pre = predict1(mnist.test.images,p_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1XswpaaAUTL"
   },
   "source": [
    "model2에 대한 predict 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EryitGQ-1cdr"
   },
   "outputs": [],
   "source": [
    "pre2 = predict2(mnist.test.images,p_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtrNt8HE8v3_"
   },
   "source": [
    "model3에 대한 predict 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j990tdot8vbb"
   },
   "outputs": [],
   "source": [
    "pre3 = predict3(mnist.test.images,p_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DybeHIxWAW5P"
   },
   "source": [
    "predictions를 통해 predict 값을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yJKLwFRtfDQ"
   },
   "outputs": [],
   "source": [
    "# Test model and check accuracy\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros([test_size, 10])\n",
    "\n",
    "predictions = pre+pre2+pre3\n",
    "\n",
    "ensemble_correct_prediction = tf.equal( tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean( tf.cast(ensemble_correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6tvX7XjAeX2"
   },
   "source": [
    "ensemble 정확도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2301075,
     "status": "ok",
     "timestamp": 1544377980466,
     "user": {
      "displayName": "김현우",
      "photoUrl": "",
      "userId": "07383167550908655617"
     },
     "user_tz": -540
    },
    "id": "ajbYINu15tWF",
    "outputId": "70703a01-3840-461b-a9b8-5db36dc7b143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble accuracy: 0.9941\n"
     ]
    }
   ],
   "source": [
    "print ('Ensemble accuracy:', sess.run(ensemble_accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_CNN_Ensembleipynb",
   "provenance": [
    {
     "file_id": "1UujDuXUBbtPpMDS_1SAm-xKHDZpA_v5E",
     "timestamp": 1544297633547
    },
    {
     "file_id": "10ZqrvbHlFlwznQ0SsThpUwAbEElUv_q_",
     "timestamp": 1544281362851
    },
    {
     "file_id": "1ru87RoDyrn5zmKl4a528fDbWmWz2nTRJ",
     "timestamp": 1543937778964
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
